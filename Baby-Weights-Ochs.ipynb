{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Weight Prediction Project\n",
    "## by Oliver Ochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“A new baby's gender, name, time of birth, and birth weight are nice information for a birth announcement, but birth weight is especially important for an obstetrician. A large size at delivery has long been associated with an increased risk of injuries to a newborn and its mom. So the better a doctor can predict birth weight, the easier the delivery may be.” - WebMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the goal being to see what type of regression model works best on with dataset, this project tests time and performance of training models in order to predict newborn babies weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from CSV and describe variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID : Unique Identification number of a baby\n",
      "SEX : Sex of the baby\n",
      "MARITAL: Marital status of its parents\n",
      "FAGE : Age of father\n",
      "GAINED : Weight gained during pregnancy\n",
      "VISITS : Number of prenatal visits\n",
      "MAGE : Age of mother\n",
      "FEDUC : Father's years of education\n",
      "MEDUC : Mother's years of education\n",
      "TOTALP : Total pregnancies\n",
      "BDEAD : number of children born alive now dead\n",
      "TERMS : Number of other terminations \n",
      "LOUTCOME : Outcome of last delivery\n",
      "WEEKS : Completed weeks of gestation\n",
      "RACEMOM : Race of mother/child from this set {0: 'Unknown',1:'OTHER_NON_WHITE', 2:'WHITE', 3:'BLACK', 4:'AMERICAN_INDIAN', 5:'CHINESE', 6:'JAPANESE', 7:'HAWAIIAN', 8:'FILIPINO', 9:'OTHER_ASIAN'}\n",
      "RACEDAD : Race of Father from this set {0:'Unknown',1:'OTHER_NON_WHITE', 2:'WHITE', 3:'BLACK', 4:'AMERICAN_INDIAN', 5:'CHINESE', 6:'JAPANESE', 7:'HAWAIIAN', 8:'FILIPINO', 9:'OTHER_ASIAN'}\n",
      "HISPMOM : Hispanic from this set {C:Cubans, M:Mexicans,  N:No, O:Colombians P:Peruvians, S:Salvadorans, U:Guatemalans }\n",
      "HISPDAD : Is dad hispanic? from this set {C:Cubans, M:Mexicans,  N:No, O:Colombians P:Peruvians, S:Salvadorans, U:Guatemalans }\n",
      "CIGNUM : Average number of cigarettes used daily (Mother)\n",
      "DRINKNUM: Average number of drinks used daily (mother)\n",
      "ANEMIA : Mother has/had anemia\n",
      "CARDIAC : Mother has/had cardiac disease\n",
      "ACLUNG : Mother has/had acute or chronic lung disease\n",
      "DIABETES : Mother has/had diabetes\n",
      "HERPES : Mother has/had genital herpes\n",
      "HYDRAM : Mother has/had hydramnios/Oligohydramnios\n",
      "HEMOGLOB : Mother has/had hemoglobinopathy\n",
      "HYPERCH : Mother has/had chronic hypertension\n",
      "HYPERPR : mother has/had pregnancy hypertension\n",
      "ECLAMP : Mother has/had Eclampsia\n",
      "CERVIX : Mother has/had incompetent cervix\n",
      "PINFANT : Mother had/had previous infant 4000+ grams\n",
      "PRETERM : Mother has/had previus preterm/small infant\n",
      "RENAL : Mother has/had renal disease\n",
      "RHSEN : Mother has/had Rh sensitization\n",
      "UTERINE : Mother has/had uterine bleeding\n",
      "BWEIGHT :  Baby's weight at birth\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# description of dataset values\n",
    "f = open('data-description.txt', 'r')\n",
    "file_contents = f.read()\n",
    "print(file_contents)\n",
    "f.close()\n",
    "\n",
    "# dataset\n",
    "data = pd.read_csv(\"baby-weights-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistics on BWEIGHT variable in dataset\n",
      "mean: [7.25806583]\n",
      "standard deviation: [1.32946068]\n",
      "min: [0.1875]\n",
      "max: [13.0625]\n",
      "25th percentile: [6.625]\n",
      "median: [7.375]\n",
      "75th percentile: [8.0625]\n"
     ]
    }
   ],
   "source": [
    "def initial_stats():\n",
    "    # Computed mean, stdev, min, max, 25% percentile, median and 75% percentile of BWEIGHT target variable\n",
    "    cols = ['ID', 'SEX', 'MARITAL', 'FAGE', 'GAINED', 'VISITS', 'MAGE', 'FEDUC', 'MEDUC', 'TOTALP', 'BDEAD',\n",
    "            'TERMS', 'LOUTCOME', 'WEEKS', 'RACEMOM', 'RACEDAD', 'HISPMOM', 'HISPDAD', 'CIGNUM', 'DRINKNUM',\n",
    "            'ANEMIA', 'CARDIAC', 'ACLUNG', 'DIABETES', 'HERPES', 'HYDRAM', 'HEMOGLOB', 'HYPERCH', 'HYPERPR',\n",
    "            'ECLAMP', 'CERVIX', 'PINFANT', 'PRETERM', 'RENAL', 'RHSEN', 'UTERINE', 'BWEIGHT']\n",
    "    mean = data[['BWEIGHT']].mean()\n",
    "    std = data[['BWEIGHT']].std()\n",
    "    min = data[['BWEIGHT']].min()\n",
    "    max = data[['BWEIGHT']].max()\n",
    "    tf = data[['BWEIGHT']].quantile(.25)\n",
    "    med = data[['BWEIGHT']].median()\n",
    "    sf = data[['BWEIGHT']].quantile(.75)\n",
    "    result = np.array([mean, std, min, max, tf, med, sf])\n",
    "    return result\n",
    "print(\"Initial statistics on BWEIGHT variable in dataset\")\n",
    "print(\"mean:\", initial_stats()[0])\n",
    "print(\"standard deviation:\", initial_stats()[1])\n",
    "print(\"min:\", initial_stats()[2])\n",
    "print(\"max:\", initial_stats()[3])\n",
    "print(\"25th percentile:\", initial_stats()[4])\n",
    "print(\"median:\", initial_stats()[5])\n",
    "print(\"75th percentile:\", initial_stats()[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate data to ensure proper functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_to_num(full_dataset):\n",
    "    # Takes full_dataset (Pandas dataframe) as input, and returns a revised\n",
    "    # full_dataset Dataframe after replacing all the non-numeric variables (i.e.,\n",
    "    # categorical variables) with mapped numeric encoding.\n",
    "    \n",
    "    tonum = {\"HISPDAD\": {\"C\": 1, \"M\": 2, \"N\": 3, \"O\": 4,\n",
    "                         \"P\": 5, \"S\": 6, \"U\": 7},\n",
    "             \"HISPMOM\": {\"C\": 1, \"M\": 2, \"N\": 3, \"O\": 4,\n",
    "                         \"P\": 5, \"S\": 6, \"U\": 7}\n",
    "             }\n",
    "    full_dataset.replace(tonum, inplace=True)\n",
    "    return full_dataset\n",
    "num_data = data_to_num(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values Filled\n",
      "ID          0\n",
      "SEX         0\n",
      "MARITAL     0\n",
      "FAGE        0\n",
      "GAINED      1\n",
      "VISITS      0\n",
      "MAGE        0\n",
      "FEDUC       1\n",
      "MEDUC       0\n",
      "TOTALP      0\n",
      "BDEAD       0\n",
      "TERMS       0\n",
      "LOUTCOME    0\n",
      "WEEKS       1\n",
      "RACEMOM     0\n",
      "RACEDAD     0\n",
      "HISPMOM     0\n",
      "HISPDAD     0\n",
      "CIGNUM      1\n",
      "DRINKNUM    0\n",
      "ANEMIA      0\n",
      "CARDIAC     0\n",
      "ACLUNG      0\n",
      "DIABETES    0\n",
      "HERPES      0\n",
      "HYDRAM      1\n",
      "HEMOGLOB    0\n",
      "HYPERCH     0\n",
      "HYPERPR     0\n",
      "ECLAMP      0\n",
      "CERVIX      0\n",
      "PINFANT     0\n",
      "PRETERM     0\n",
      "RENAL       0\n",
      "RHSEN       0\n",
      "UTERINE     0\n",
      "BWEIGHT     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def fill_nan(full_dataset):\n",
    "    # Given the full_dataset (Pandas Dataframe), checks if there are missing values, and if yes,\n",
    "    # counts how many, and impute the missing values with corresponding mean values.\n",
    "    # Finally, returns the counting result as a Pandas dataframe with 2 columns\n",
    "    # {variable_name,num_of_missing_values). Also, returns the revised full_dataset after the missing\n",
    "    # value imputations is done.\n",
    "    import pandas as pd\n",
    "\n",
    "    missing_count = pd.DataFrame()\n",
    "    revised_full_dataset = pd.DataFrame()\n",
    "\n",
    "    missing_count = full_dataset.isnull().sum()\n",
    "    revised_full_dataset = full_dataset.fillna(full_dataset.mean())\n",
    "    return (missing_count, revised_full_dataset)\n",
    "revised_num_dataset = fill_nan(num_data)\n",
    "print(\"Missing values Filled\")\n",
    "print(revised_num_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of features to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow0_col0 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow1_col0 {\n",
       "            background-color:  #a7c5fe;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow2_col0 {\n",
       "            background-color:  #8db0fe;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow3_col0 {\n",
       "            background-color:  #688aef;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow4_col0 {\n",
       "            background-color:  #6788ee;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow5_col0 {\n",
       "            background-color:  #6180e9;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow6_col0 {\n",
       "            background-color:  #5f7fe8;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow7_col0 {\n",
       "            background-color:  #5e7de7;\n",
       "            color:  #000000;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow8_col0 {\n",
       "            background-color:  #485fd1;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow9_col0 {\n",
       "            background-color:  #445acc;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow10_col0 {\n",
       "            background-color:  #4358cb;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow11_col0 {\n",
       "            background-color:  #4358cb;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow12_col0 {\n",
       "            background-color:  #4257c9;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow13_col0 {\n",
       "            background-color:  #3f53c6;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow14_col0 {\n",
       "            background-color:  #3e51c5;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow15_col0 {\n",
       "            background-color:  #3d50c3;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow16_col0 {\n",
       "            background-color:  #3d50c3;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow17_col0 {\n",
       "            background-color:  #3c4ec2;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow18_col0 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_31c6382c_d27e_11ea_b91b_b37086b3950drow19_col0 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950d\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >BWEIGHT</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row0\" class=\"row_heading level0 row0\" >WEEKS</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow0_col0\" class=\"data row0 col0\" >0.565373</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row1\" class=\"row_heading level0 row1\" >GAINED</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow1_col0\" class=\"data row1 col0\" >0.173262</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row2\" class=\"row_heading level0 row2\" >VISITS</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow2_col0\" class=\"data row2 col0\" >0.129587</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row3\" class=\"row_heading level0 row3\" >MAGE</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow3_col0\" class=\"data row3 col0\" >0.068473</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row4\" class=\"row_heading level0 row4\" >PINFANT</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow4_col0\" class=\"data row4 col0\" >0.067073</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row5\" class=\"row_heading level0 row5\" >MEDUC</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow5_col0\" class=\"data row5 col0\" >0.055908</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row6\" class=\"row_heading level0 row6\" >FEDUC</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow6_col0\" class=\"data row6 col0\" >0.052674</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row7\" class=\"row_heading level0 row7\" >FAGE</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow7_col0\" class=\"data row7 col0\" >0.051447</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row8\" class=\"row_heading level0 row8\" >DIABETES</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow8_col0\" class=\"data row8 col0\" >0.010216</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row9\" class=\"row_heading level0 row9\" >TOTALP</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow9_col0\" class=\"data row9 col0\" >0.003201</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row10\" class=\"row_heading level0 row10\" >RHSEN</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow10_col0\" class=\"data row10 col0\" >0.001985</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row11\" class=\"row_heading level0 row11\" >HERPES</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow11_col0\" class=\"data row11 col0\" >0.001442</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row12\" class=\"row_heading level0 row12\" >CARDIAC</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow12_col0\" class=\"data row12 col0\" >-0.000425</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row13\" class=\"row_heading level0 row13\" >ACLUNG</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow13_col0\" class=\"data row13 col0\" >-0.006643</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row14\" class=\"row_heading level0 row14\" >RENAL</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow14_col0\" class=\"data row14 col0\" >-0.007734</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row15\" class=\"row_heading level0 row15\" >DRINKNUM</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow15_col0\" class=\"data row15 col0\" >-0.010207</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row16\" class=\"row_heading level0 row16\" >HEMOGLOB</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow16_col0\" class=\"data row16 col0\" >-0.010994</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row17\" class=\"row_heading level0 row17\" >ANEMIA</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow17_col0\" class=\"data row17 col0\" >-0.011487</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row18\" class=\"row_heading level0 row18\" >HISPDAD</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow18_col0\" class=\"data row18 col0\" >-0.014481</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950dlevel0_row19\" class=\"row_heading level0 row19\" >HISPMOM</th>\n",
       "                        <td id=\"T_31c6382c_d27e_11ea_b91b_b37086b3950drow19_col0\" class=\"data row19 col0\" >-0.015853</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f8c69c2508>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correlated_vars(full_dataset_all_numeric):\n",
    "    # Takes in full_dataset (Pandas dataframe) where all the categorical variables are\n",
    "    # already replaced with numeric values, returns a list of top 20 highly correlated variables\n",
    "    # (with respect to the target variable) as a Pandas dataframe with 2 columns {variable,corr_score}.\n",
    "    # The corr_score between a variable x and the target variable y is computed using the\n",
    "    # Pearson Correlation Coefficient.\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    data = full_dataset_all_numeric.corr(method='pearson')['BWEIGHT'][1:-1]\n",
    "    result = data.nlargest(20)\n",
    "\n",
    "    return result.to_frame()\n",
    "kept_vars = correlated_vars(revised_num_dataset[1])\n",
    "kept_vars.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate variables and split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset(full_dataset):\n",
    "    # Separates the full_dataset into two parts: X and y, where X denotes the input matrix\n",
    "    # containing only the input variables, and y denotes the target vector containing only the target values\n",
    "    # for exactly the same number of samples in the given full_dataset (pandas Dataframe).\n",
    "    # Finally, returns X and y as a tuple\n",
    "    import pandas as pd\n",
    "\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.DataFrame()\n",
    "    assert (len(X) == len(y))\n",
    "\n",
    "    X = full_dataset[full_dataset.columns[:-1]]\n",
    "    y = full_dataset['BWEIGHT']\n",
    "    return (X, y)\n",
    "separated_dataset = separate_dataset(revised_num_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_dataset(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=45931)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "dataset_split = split_dataset(*separated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(X_train, X_test, y_train, y_test):\n",
    "    # Given the 4 splits denoting the training and test dataset,\n",
    "    # Applies min-max scaling on the training dataset (X_train).\n",
    "    # Then scales the test dataset based on the metrics obtained when scaling the training dataset.\n",
    "    # Finally, returns as a tuple the scaled X_train, X_test and the intact y_train and y_test.\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    cols = list(X_train)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=cols)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=cols)\n",
    "\n",
    "\n",
    "    return (X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "min_max_scaled_dataset = min_max_scaler(*dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_test, y_train, y_test):\n",
    "    # Given the 4 splits denoting the training and test dataset,\n",
    "    # Applies standardization scaling on the training dataset (X_train).\n",
    "    # Then scales the test dataset based on the metrics obtained when scaling the training dataset.\n",
    "    # Finally, returns as a tuple the scaled X_train, X_test and the intact y_train and y_test.\n",
    "    from sklearn import preprocessing\n",
    "    cols = list(X_train)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=cols)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=cols)\n",
    "\n",
    "    return (X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "standardized_data = standardize(*dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression models and their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_close_form(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    # Given the (X_train, y_train) pairs denoting input matrix and output vector respectively,\n",
    "    # Fits a linear regression model using the close-form solution to obtain\n",
    "    # the coefficients, beta's, as a numpy array of m+1 values.\n",
    "    # Then using the computed beta values, predicts the test samples provided in the \"X_test_scaled\"\n",
    "    # argument. Computes Root Mean Squared Error (RMSE) of the prediction.\n",
    "    # Finally, returns the beta vector, y_pred, RMSE as a tuple.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    beta = []\n",
    "    y_pred = []\n",
    "    RMSE = -1\n",
    "\n",
    "    X = X_train_scaled.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "    Xt = np.transpose(X)\n",
    "    XtX = np.dot(Xt, X)\n",
    "    Xty = np.dot(Xt, y)\n",
    "    beta = np.linalg.solve(XtX, Xty)\n",
    "    for Xtest, Ytest in zip(X_test_scaled.to_numpy(), y_test.to_numpy()):\n",
    "        pred = np.dot(Xtest, beta)\n",
    "        y_pred.append(pred)\n",
    "    RMSE = np.sqrt(np.square(np.subtract(y_test, y_pred)).mean())\n",
    "    return (beta, y_pred, RMSE)\n",
    "\n",
    "#http://www2.lawrence.edu/fast/GREGGJ/Python/numpy/numpyLA.html Referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "Root Mean Squared Error (RMSE): 1.042421813184601\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "Root Mean Squared Error (RMSE): 7.329759432596651\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE):\", fit_close_form(*min_max_scaled_dataset)[2])\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE):\", fit_close_form(*standardized_data)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(X_train_scaled, X_test_scaled, y_train, y_test, learning_rate=0.001, nIteration=10):\n",
    "    # Given the (X_train, y_train) pairs denoting input matrix and output vector respectively,\n",
    "    # Fits a linear regression model using the batch gradient descent algorithm to obtain\n",
    "    # the coefficients, beta's, as a numpy array of m+1 values.\n",
    "    # Measures the cpu_time needed during the training step. cpu_time is not equal to the wall_time.\n",
    "    # Then using the computed beta values, predicts the test samples provided in the \"X_test_scaled\"\n",
    "    # argument. Computes Root Mean Squared Error (RMSE) of the prediction.\n",
    "    # Finally, returns the beta vector, y_pred, RMSE, cpu_time as a tuple.\n",
    "    from time import perf_counter\n",
    "    import numpy as np\n",
    "    import random\n",
    "    random.seed(554433)\n",
    "    beta = []\n",
    "    y_pred = []\n",
    "    RMSE = -1\n",
    "    cpu_time = 0\n",
    "\n",
    "    t_start = perf_counter()\n",
    "    X = X_train_scaled.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "    Xt = np.transpose(X)\n",
    "    beta = np.random.uniform(0, 1, 36)\n",
    "    m = len(y)\n",
    "    for i in range(nIteration):\n",
    "        beta = beta - learning_rate * (Xt.dot(X.dot(beta)-y)/m)\n",
    "    t_stop = perf_counter()\n",
    "    cpu_time = t_stop - t_start\n",
    "    for Xtest, Ytest in zip(X_test_scaled.to_numpy(), y_test.to_numpy()):\n",
    "        pred = np.dot(Xtest, beta)\n",
    "        y_pred.append(pred)\n",
    "    RMSE = np.sqrt(np.square(np.subtract(y_test, y_pred)).mean())\n",
    "    return (beta, y_pred, RMSE, cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "Root Mean Squared Error (RMSE):  4.341054723971213\n",
      "Runtime: 0.03177400000000041\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "Root Mean Squared Error (RMSE):  8.271494492845612\n",
      "Runtime: 0.02924289999999985\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_batch(*min_max_scaled_dataset)[2])\n",
    "print(\"Runtime:\", fit_batch(*min_max_scaled_dataset)[3])\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_batch(*standardized_data)[2])\n",
    "print(\"Runtime:\", fit_batch(*standardized_data)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stochastic(X_train_scaled, X_test_scaled, y_train, y_test, learning_rate=0.001, nIteration=7000):\n",
    "    # Given the (X_train, y_train) pairs denoting input matrix and output vector respectively,\n",
    "    # Fits a linear regression model using the stochastic gradient descent algorithm to obtain\n",
    "    # the coefficients, beta's, as a numpy array of m+1 values.\n",
    "    # Then using the computed beta values, predicts the test samples provided in the \"X_test_scaled\"\n",
    "    # argument. Computes Root Mean Squared Error (RMSE) of the prediction.\n",
    "    # Finally, returns the beta vector, y_pred, RMSE, cpu_time as a tuple.\n",
    "    import numpy as np\n",
    "    from time import perf_counter\n",
    "    import random\n",
    "    random.seed(554433)\n",
    "    beta = []\n",
    "    y_pred = []\n",
    "    RMSE = -1\n",
    "    cpu_time = 0\n",
    "\n",
    "    t_start = perf_counter()\n",
    "    X = X_train_scaled.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "    beta = np.random.uniform(0,1,36)\n",
    "    m = len(y)\n",
    "    for i in range(nIteration):\n",
    "        randomize = np.random.permutation(len(X))\n",
    "        X = X[randomize]\n",
    "        y = y[randomize]\n",
    "        Xt = np.transpose(X)\n",
    "        beta = beta - learning_rate * (Xt.dot(X.dot(beta) - y) / m)\n",
    "    t_stop = perf_counter()\n",
    "    cpu_time = t_stop - t_start\n",
    "    for Xtest, Ytest in zip(X_test_scaled.to_numpy(), y_test.to_numpy()):\n",
    "        pred = np.dot(Xtest, beta)\n",
    "        y_pred.append(pred)\n",
    "    RMSE = np.sqrt(np.square(np.subtract(y_test, y_pred)).mean())\n",
    "    return (beta, y_pred, RMSE, cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "Root Mean Squared Error (RMSE):  1.465083599936269\n",
      "Runtime: 240.18791009999998\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "Root Mean Squared Error (RMSE):  7.330681444886723\n",
      "Runtime: 216.32823570000005\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_stochastic(*min_max_scaled_dataset)[2])\n",
    "print(\"Runtime:\", fit_stochastic(*min_max_scaled_dataset)[3])\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_stochastic(*standardized_data)[2])\n",
    "print(\"Runtime:\", fit_stochastic(*standardized_data)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mini_batch(X_train_scaled, X_test_scaled, y_train, y_test, batch_size=64, learning_rate=0.001, nIteration=200):\n",
    "    # Given the (X_train, y_train) pairs denoting input matrix and output vector respectively,\n",
    "    # Fits a linear regression model using the mini-batch gradient descent algorithm to obtain\n",
    "    # the coefficients, beta's, as a numpy array of m+1 values.\n",
    "    # Then using the computed beta values, predicts the test samples provided in the \"X_test_scaled\"\n",
    "    # argument. Computes Root Mean Squared Error (RMSE) of the prediction.\n",
    "    # Finally, returns the beta vector, y_pred, RMSE, cpu_time as a tuple.\n",
    "    from time import perf_counter\n",
    "    import numpy as np\n",
    "    import random\n",
    "    random.seed(554433)\n",
    "    beta = []\n",
    "    y_pred = []\n",
    "    RMSE = -1\n",
    "    cpu_time = 0\n",
    "\n",
    "    t_start = perf_counter()\n",
    "    X = X_train_scaled.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "    beta = np.random.uniform(0,1,36)\n",
    "    m = len(y)\n",
    "    for i in range(nIteration):\n",
    "        randomize = np.random.permutation(len(X))\n",
    "        X = X[randomize]\n",
    "        y = y[randomize]\n",
    "        Xt = np.transpose(X)\n",
    "        for j in range(batch_size):\n",
    "            beta = beta - learning_rate * (Xt.dot(X.dot(beta) - y) / m)\n",
    "    t_stop = perf_counter()\n",
    "    cpu_time = t_stop - t_start\n",
    "    for Xtest, Ytest in zip(X_test_scaled.to_numpy(), y_test.to_numpy()):\n",
    "        pred = np.dot(Xtest, beta)\n",
    "        y_pred.append(pred)\n",
    "    RMSE = np.sqrt(np.square(np.subtract(y_test, y_pred)).mean())\n",
    "\n",
    "    return (beta, y_pred, RMSE, cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "Root Mean Squared Error (RMSE):  1.3468882811489113\n",
      "Runtime: 27.62068050000005\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "Root Mean Squared Error (RMSE):  7.329615512424052\n",
      "Runtime: 30.42870019999998\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_mini_batch(*min_max_scaled_dataset)[2])\n",
    "print(\"Runtime:\", fit_mini_batch(*min_max_scaled_dataset)[3])\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(\"Root Mean Squared Error (RMSE): \", fit_mini_batch(*standardized_data)[2])\n",
    "print(\"Runtime:\", fit_mini_batch(*standardized_data)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_results(batch_GD_result, stochastic_GD_result, minibatch_GD_result):\n",
    "    # Given the 3 sets of tuples from the 3 experiments with batch gradient descent,\n",
    "    # stochastic gradient descent and mini-batch gradient descent, returns a string from the set\n",
    "    # {\"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the best predictive performance\n",
    "    # in terms of RMSE.\n",
    "\n",
    "    (beta_B, y_pred_B, RMSE_B, cpu_time_B) = batch_GD_result\n",
    "    (beta_S, y_pred_S, RMSE_S, cpu_time_S) = stochastic_GD_result\n",
    "    (beta_M, y_pred_M, RMSE_M, cpu_time_M) = minibatch_GD_result\n",
    "\n",
    "    RMSEs = {'Batch': RMSE_B, 'Stochastic': RMSE_S, 'Mini': RMSE_M}\n",
    "    minpred = min(RMSEs.keys(), key=(lambda k: RMSEs[k]))\n",
    "    return ('Best prediction by: ' + str(minpred) + ' with ' + str(RMSEs[minpred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "Best prediction by: Mini with 1.3827329243324744\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "Best prediction by: Mini with 7.329587472973014\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(predictive_results(fit_batch(*min_max_scaled_dataset), fit_stochastic(*min_max_scaled_dataset), fit_mini_batch(*min_max_scaled_dataset)))\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(predictive_results(fit_batch(*standardized_data), fit_stochastic(*standardized_data), fit_mini_batch(*standardized_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_results(batch_GD_result, stochastic_GD_result, minibatch_GD_result):\n",
    "    # Given the 3 sets of tuples from the 3 experiments with batch gradient descent,\n",
    "    # stochastic gradient descent and mini-batch gradient descent, returns a string from the set\n",
    "    # {\"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the least training time.\n",
    "\n",
    "    (beta_B, y_pred_B, RMSE_B, cpu_time_B) = batch_GD_result\n",
    "    (beta_S, y_pred_S, RMSE_S, cpu_time_S) = stochastic_GD_result\n",
    "    (beta_M, y_pred_M, RMSE_M, cpu_time_M) = minibatch_GD_result\n",
    "\n",
    "    cpu_times = {'Batch': cpu_time_B, 'Stochastic': cpu_time_S, 'minibatch': cpu_time_M}\n",
    "\n",
    "    mintime = min(cpu_times.keys(), key=(lambda k: cpu_times[k]))\n",
    "    return ('min runtime by: ' + str(mintime) + ' with ' + str(cpu_times[mintime]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalization of features\n",
      "min runtime by: Batch with 0.025800699999990684\n",
      "\n",
      "\n",
      "Standardization of features\n",
      "min runtime by: Batch with 0.020153799999889088\n"
     ]
    }
   ],
   "source": [
    "print(\"Min-Max Normalization of features\")\n",
    "print(time_results(fit_batch(*min_max_scaled_dataset), fit_stochastic(*min_max_scaled_dataset), fit_mini_batch(*min_max_scaled_dataset)))\n",
    "print(\"\\n\")\n",
    "print(\"Standardization of features\")\n",
    "print(time_results(fit_batch(*standardized_data), fit_stochastic(*standardized_data), fit_mini_batch(*standardized_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
